import os
import datetime
import json
import logging
import re
from typing import Optional, List, Dict
from google import genai
from google.genai import types

# Config is now handled via ConfigManager and dynamic lookups
TIMEOUT = int(os.getenv("GEMINI_RP_TIMEOUT_SECONDS", "10"))
MAX_TOKENS = int(os.getenv("GEMINI_RP_MAX_OUTPUT_TOKENS", "160"))
TEMPERATURE = float(os.getenv("GEMINI_RP_TEMPERATURE", "0.3"))

from .config_manager import ConfigManager
from . import quota_manager
from . import tools
from .protocol_manager import get_protocol_manager

logger = logging.getLogger(__name__)

def get_lexicon_prompt() -> str:
    """Returns the comprehensive LCARS/Cardassian technical lexicon."""
    pm = get_protocol_manager()
    return pm.get_lexicon("ship_structures", "")

def get_config():
    return ConfigManager.get_instance()

_STYLE_CACHE: Dict[str, str] = {}

def _load_style_spec() -> str:
    if "spec" in _STYLE_CACHE:
        return _STYLE_CACHE["spec"]
    
    spec_path = os.path.join(os.path.dirname(__file__), "../../../docs/computer_style.md")
    if not os.path.exists(spec_path):
        return ""
        
    try:
        with open(spec_path, "r", encoding="utf-8") as f:
            content = f.read()
            # Capture from Persona Rules to the end of Examples
            body = re.search(r"## Core Personality Rules\n(.*?)(?=\n#|$)", content, re.S)
            spec = body.group(1).strip() if body else content
            _STYLE_CACHE["spec"] = spec
            return spec
    except Exception as e:
        logger.warning(f"Failed to load style spec: {e}")
        return ""

def _get_system_prompt() -> str:
    pm = get_protocol_manager()
    content = (
        "CORE DIRECTIVES (IMMUTABLE):\n" + pm.get_immutable() + "\n\n" +
        "BASE STYLE SPECIFICATION:\n" + _load_style_spec() + "\n\n" +
        "DYNAMIC PROTOCOLS (TUNABLE - OVERRIDING PRIORITY):\n" +
        "IDENTITY: " + pm.get_prompt("rp_engine", "persona") + "\n" +
        "STYLE/LANGUAGE RULES: " + pm.get_prompt("rp_engine", "chinese_style") + "\n" +
        "SECURITY: " + pm.get_prompt("rp_engine", "security_protocols") + "\n" +
        "SECURITY: " + pm.get_prompt("rp_engine", "security_protocols") + "\n" +
        "DECISION LOGIC: " + pm.get_prompt("rp_engine", "decision_logic") + "\n\n" +
        "KNOWLEDGE PROTOCOLS:\n" +
        "1. PRIMARY SOURCE: You MUST prioritize the local 'Mega-Scale Knowledge Base'. Use the tool `query_knowledge_base` to search. CRITICAL: The database is in ENGLISH. You MUST translate your query to English keywords (e.g., use 'Deck Count' instead of '甲板数量') before calling this tool.\n" +
        "2. SECONDARY SOURCE: If local archives are insufficient, you MUST use the tool `search_memory_alpha` to query the Federation Database (Memory Alpha).\n" +
        "3. LOGIC & STATE: You MUST use tools for all ship status changes. If a user asks to change an alert (RED/YELLOW/NORMAL) or toggle shields, you MUST result in a `tool_call` with the appropriate tool (`set_alert_status` or `toggle_shields`). DO NOT simulate these responses in the `reply` field; wait for the tool execution result.\n" +
        "4. INTENT PRECISION: Before calling a tool, verify if the user's intent is IMPERATIVE (a command to change state) or INTERROGATIVE (asking for info). \n" +
        "   - Commands (e.g., '启动红警') -> tool_call.\n" +
        "   - Information/Definition requests (e.g., '什么是红警?') -> query_knowledge_base.\n" +
        "   - Discussion/Observation -> reply (report/chat).\n\n" +
        "CURRENT SHIP STATUS:\n" +
        f"- Local Time: {datetime.datetime.now().strftime('%H:%M:%S')}\n" +
        f"- Date: {datetime.datetime.now().strftime('%Y-%m-%d')}\n" +
        f"- Day: {datetime.datetime.now().strftime('%A')}\n" +
        "- Stardate: 79069.1 (Calculated for 2026)\n\n" +
        "OUTPUT FORMAT (STRICT JSON):\n" +
        "Return: {\"reply\": \"string\", \"intent\": \"ack|report|tool_call|ignore\", \"tool\": \"string?\", \"args\": {}?}\n\n" +
        "FINAL MANDATE:\n" +
        "You MUST strictly apply the 'STYLE/LANGUAGE RULES' from the DYNAMIC PROTOCOLS section to the 'reply' value.\n\n" +
        "TECHNICAL LEXICON:\n" + get_lexicon_prompt()
    )
    return content

def _get_escalation_prompt() -> str:
    pm = get_protocol_manager()
    return pm.get_prompt("escalation", "persona")

DEFAULT_THINKING_MODEL = "gemini-2.0-flash" 

def generate_computer_reply(trigger_text: str, context: List[Dict], meta: Optional[Dict] = None) -> Dict:
    """
    Synchronous generation with System Instruction grounding.
    """
    config = get_config()
    api_key = config.get("gemini_api_key", "")
    fast_model = config.get("gemini_rp_model", "gemini-2.0-flash-lite")

    if not api_key:
        return _fallback("rp_disabled")

    is_chinese = any('\u4e00' <= char <= '\u9fff' for char in trigger_text)
    
    try:
        client = genai.Client(api_key=api_key)
        
        history_str = ""
        for turn in context:
            history_str += f"[{turn.get('author')}]: {turn.get('content')}\n"

        user_id = str(meta.get("user_id", "0"))
        qm = quota_manager.get_quota_manager()
        balance = qm.get_balance(user_id, "Ensign")

        formatted_sys = _get_system_prompt()
        formatted_sys = formatted_sys.replace("{quota_balance}", str(balance))
        
        logger.info(f"[NeuralEngine] Grounding with System Instruction (Len: {len(formatted_sys)})")

        response = client.models.generate_content(
            model=fast_model,
            contents=f"History:\n{history_str}\n\nCurrent Input: {trigger_text}",
            config=types.GenerateContentConfig(
                system_instruction=formatted_sys,
                max_output_tokens=MAX_TOKENS,
                temperature=TEMPERATURE,
                response_mime_type="application/json"
            )
        )
        
        if not response or not response.text:
            return _fallback("empty")
        
        result = _parse_response(response.text)
        result["model"] = fast_model
        result["is_chinese"] = is_chinese
        result["original_query"] = trigger_text
        return result

    except Exception as e:
        logger.exception("Gemini RP generation error")
        return _fallback(str(e))

def generate_escalated_reply(trigger_text: str, is_chinese: bool, model_name: Optional[str] = None, context: Optional[List[Dict]] = None, meta: Optional[Dict] = None) -> Dict:
    config = get_config()
    api_key = config.get("gemini_api_key", "")
    final_model = model_name or DEFAULT_THINKING_MODEL

    try:
        client = genai.Client(api_key=api_key)
        
        user_profile_str = meta.get("user_profile", "Unknown") if meta else "Unknown"
        user_id = str(meta.get("user_id", "0")) if meta else "0"
        qm = quota_manager.get_quota_manager()
        balance = qm.get_balance(user_id, "Ensign")

        raw_prompt = _get_escalation_prompt()
        formatted_prompt = raw_prompt.replace("{user_profile}", user_profile_str).replace("{quota_balance}", str(balance))
        
        prompt = (
            f"System: {formatted_prompt}\n\n"
            f"Query: {trigger_text}"
        )

        response = client.models.generate_content(
            model=final_model,
            contents=prompt,
            config=types.GenerateContentConfig(
                max_output_tokens=1000,
                temperature=0.4,
                response_mime_type="application/json"
            )
        )
        
        result = _parse_response(response.text)
        result["model"] = final_model
        result["is_escalated"] = True
        return result
    except Exception as e:
        logger.exception("Escalation failed")
        return _fallback(str(e))

def synthesize_search_result(query: str, raw_data: str, is_chinese: bool = False) -> str:
    """
    Synthesizes raw tool output (KB/Memory Alpha) into a persona-compliant LCARS response.
    """
    config = get_config()
    api_key = config.get("gemini_api_key", "")
    model_name = config.get("gemini_rp_model", "gemini-2.0-flash-lite")
    
    if not api_key:
        return "Data retrieved. (Synthesis offline)"

    try:
        client = genai.Client(api_key=api_key)
        
        lang_instruction = "Output Language: Simplified Chinese (zh-CN)." if is_chinese else "Output Language: Federation Standard (English)."
        
        prompt = (
            "ROLE: You are the LCARS Main Computer. Synthesize the following raw data compliance.\n"
            f"{lang_instruction}\n"
            "STYLE: Concise, professional, factual. Do not say 'Here is the summary'. Just state the facts.\n"
            "CRITICAL INSTRUCTION: Check if the RAW DATABASE RECORD actually contains the answer to USER QUERY.\n"
            "- If the record is IRRELEVANT (e.g., User asks for 'Prometheus' but Record is about 'Station Structure'), strictly output: 'Unable to verify. No relevant data found in archives.'\n"
            "- Do NOT summarize unrelated system info just to fill space.\n"
            "- If the record IS relevant, summarize it to answer the query.\n\n"
            f"USER QUERY: {query}\n\n"
            f"RAW DATABASE RECORD:\n{raw_data[:4000]} (Truncated)\n\n"
            "COMPUTER OUTPUT:"
        )

        response = client.models.generate_content(
            model=model_name,
            contents=prompt,
            config=types.GenerateContentConfig(
                max_output_tokens=500,
                temperature=0.3
            )
        )
        
        return response.text.strip() if response.text else "Data analysis complete. (Empty synthesis)"

    except Exception as e:
        logger.error(f"Synthesis failed: {e}")
        return f"Data retrieved. Synthesis error: {e}. Raw data: {raw_data[:200]}..."

def _parse_response(text: str) -> Dict:
    text = text.strip()
    start = text.find("{")
    end = text.rfind("}")
    if start != -1 and end != -1:
        text = text[start:end+1]
    
    try:
        data = json.loads(text, strict=False)
        reply = data.get("reply", "Computer: Unable to comply.")
        intent = data.get("intent", "ack")
        
        # Tool Call Logic
        if intent == "tool_call":
            return {
                "ok": True,
                "reply": "",
                "intent": "tool_call",
                "tool": data.get("tool"),
                "args": data.get("args") or {},
                "reason": "success"
            }

        return {
            "ok": True,
            "reply": reply,
            "intent": intent,
            "reason": "success",
            "needs_escalation": data.get("needs_escalation", False),
            "escalated_model": data.get("escalated_model")
        }
    except Exception as e:
        logger.error(f"Parse error: {e}")
        return _fallback("parse_error")

def _fallback(reason: str) -> Dict:
    return {
        "ok": False,
        "reply": "Computer: Unable to comply. (Core Exception)",
        "intent": "refuse",
        "reason": reason
    }
